---
title: "Stochastic search variable selection in JAGS"
description: |
  Using spike and slab priors to shrink coefficients toward zero.
author:
  - name: Maxwell B. Joseph
    url: {}
date: 2014-03-22

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Stochastic search variable selection (SSVS) identifies promising subsets of 
multiple regression covariates via Gibbs sampling [@george1993variable]. Here's 
a short SSVS demo with [JAGS](http://mcmc-jags.sourceforge.net/) and 
[R](http://www.r-project.org/).

Assume we have a multiple regression problem:

$$\boldsymbol{Y} \sim N_n(\boldsymbol{X \beta}, \sigma^2 \boldsymbol{I})$$

We suspect only a subset of the elements of $\boldsymbol{\beta}$ are non-zero, 
i.e. some of the covariates have no effect.

Assume $\boldsymbol{\beta}$ arises from one of two normal mixture components, 
depending on a latent variable $\gamma_i$:

$$
\beta_i \mid \gamma_i  \sim \left\{
  \begin{array}{lr}
    N(0, \tau^2_i) &  \gamma_i = 0\\
    N(0, c^2_i \tau^2_i) &  \gamma_i = 1
  \end{array}
\right.
$$

$\tau_i$ is positive but small s.t. $\beta_i$ is close to zero when 
$\gamma_i = 0$. $c_i$ is large enough to allow reasonable deviations from zero
when $\gamma_i = 1$. The prior probability that covariate $i$ has a nonzero 
effect is $Pr(\gamma_i = 1) = p_i$. 

Let's simulate a dataset in which some covariates have strong effects on the 
linear predictor, and other don't.

```{r simulate-data, message=FALSE}
library(gridExtra)
library(coda)
library(knitr)
ncov <- 20
nobs <- 60
var_beta <- .004
c <- 1000
p_inclusion <- .5
sigma_y <- 1
# generate covariates
X <- array(dim=c(nobs, ncov))
for (i in 1:ncov){
  X[, i] <- rnorm(nobs, 0, 1)
}
included <- rbinom(ncov, 1, p_inclusion)
coefs <- rnorm(n=ncov,
               mean=0,
               sd=ifelse(included==1,
                         sqrt(var_beta * c),
                         sqrt(var_beta)
                         )
               )
coefs <- sort(coefs)
Y <- rnorm(nobs, mean=X %*% coefs, sd=sigma_y)
```

Specifying the model:

```{r define-model}
cat("model{
  alpha ~ dnorm(0, 1)
  sd_y ~ dunif(0, 10)
  tau_y <- pow(sd_y, -2)
  # ssvs priors
  sd_bet ~ dunif(0, 10)
  tau_in <- pow(sd_bet, -2)
  tau[1] <- tau_in            # coef effectively zero
  tau[2] <- tau_in / 1000     # nonzero coef
  p_ind[1] <- 1/2
  p_ind[2] <- 1 - p_ind[1]
  for (j in 1:ncov){
    indA[j] ~ dcat(p_ind[]) # returns 1 or 2
    gamma[j] <- indA[j] - 1   # returns 0 or 1
    beta[j] ~ dnorm(0, tau[indA[j]])
  }
  # likelihood
  for (i in 1:nobs){
    Y[i] ~ dnorm(alpha + X[i ,] %*% beta[], tau_y)
  }
}
    "
    , file="ssvs.txt")
```

Fitting the model:

```{r fit-model, message=FALSE}
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)

```

Now, let's visualize the inclusion probabilities.
