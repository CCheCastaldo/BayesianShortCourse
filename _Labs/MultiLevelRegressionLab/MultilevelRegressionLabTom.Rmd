---
knit: (function(inputFile, encoding) {rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../content/labs")})
output:
  html_document:
    css: ../../_HeadersEtc/style.css
    highlight: default
    theme: paper
editor_options: 
  chunk_output_type: console
---

<script src="../../_HeadersEtc/hideOutput.js"></script>

<img src="../../_HeadersEtc/SESYNCBayes/Logo.png" style="position:absolute;top:20px;right:220px;width:150px;height=150px" />

##### `r readChar("../../_HeadersEtc/SESYNCBayes/Title.txt", file.info("../../_HeadersEtc/SESYNCBayes/Title.txt")$size)`

#### Multilevel Regression

##### `r format(Sys.Date(), format="%B %d, %Y")`

```{r preliminaries, include = FALSE}
library(knitr)
library(actuar)
library(rjags)
library(ggplot2)
library(MCMCvis)
library(HDInterval)
library(SESYNCBayes)
library(tidyverse)
knitr::opts_chunk$set(cache = FALSE)

set.seed(10)

```

---

### **Introduction**

Multi-level models are a workhorse for understanding ecological processes because so many problems contain information at nested spatial scales, levels of organization, or categories. This problem will give you practice implementing the math that you wrote in the model building exercise on N~2~O emissions from agricultural soils.  The deterministic models that we will use here are linear, but the approach applies equally well to non-linear forms. The data set that you will analyze is described in the companion document `MultilevelModelBuildingExercise.pdf`.

There are two sets of problems here, one focusing on model writing, the other focusing on writing code to implement the MCMC in JAGS.  We suggest you work this way.  Alternate between the model model writing and the model coding. Study the math, then use the math as a template for your code.  If we had more time, we would have asked you to write the models.

You need to load the `ggplot2`, `rjags`, `MCMCVis`, `SESYNCBayes` and `HDInterval` libraries. It is always a good idea to look at the data. Examine the head of the data frame for emissions. Note that the columns `group.index` and `fert.index` contain indices for sites and fertilizer types. You will need to understand how these are used in the "index trick" covered in lecture. Plot $N_2O$ emissions as a function of fertilizer input by group and fertilizer type. Set the seed to 10 compare your answers to the key.

```{r}
data(N2OEmission)
y = N2OEmission
w=SiteCarbon
names(y)
y.n.sites = length(unique(y$group))
head(y)
qplot(n.input, emission, data=y, color =  group)
qplot(n.input, emission, data=y, color =  fertilizer)

y2=y%>% group_by(group.index) %>%
  summarize(reps2 =  n()) %>% ungroup 
sum(y2$reps2)  #should = number of rows of y, 563

```

#### IV. Pooled model
Your first task is to write a simple, "pooled" model where you gloss over differences in sites and fertilizer types and lump everything into a set of $x$ & $y$ pairs using the R template provided below. It is imperative that you study the data statement and match the variable names in your JAGS code to the left hand side of the = in the data list.  Call the intercept `alpha`, the slope `beta` and use `sigma` to name the standard deviation in the likelihood.

Note that in this problem and the ones that follow I have set up the data and the initial conditions for you.  This will save time and frustration, allowing you to concentrate on writing code for the model but you must pay attention to the names I give in the `data` and `inits` lists.  These must agree with the variable names in your model.  Please see Brian  or me if there is anything that you don't understand about these lists.
```{r}
#very important to study this:
data = list(
  y.emission = log(y$emission),
  y.n.input = log(y$n.input)-mean(log(y$n.input)) #center the data to speed convergence and aid in interpretation of intercept.  Could also standardize by dividing by the standard deviation.

)
  inits = list(
  list(
    alpha = 0,
    beta = .5,
    sigma = 50
  ),
  list(
    alpha = 1,
    beta = 1.5,
    sigma = 10
  )
)

```


Write the code for the model.  Compile the model and execute the MCMC to produce a coda object. Produce trace plots of the chains for model parameters.  Produce a summary table and caterpillar for the parameters and tests for convergence including the effective sample size.

```{r, include=key, echo=key, eval=key }
####Pooled model
{
sink("PooledJAGS.R")
cat("
model{
#priors
alpha ~ dnorm(0,.0001)
beta ~ dnorm(0,.0001)
sigma ~ dunif(0,100)
tau.reg <- 1/sigma^2
#likelihood
#note that data have been log-transformed on the R side. 
 for(i in 1:length(y.emission)){
    log_mu[i] <- alpha + beta * y.n.input[i]
    y.emission[i] ~ dnorm(log_mu[i], tau.reg)
 }



}
    
",fill=TRUE)
sink()
}

```



```{r, include=key, echo=key, eval=key}
  n.adapt=3000
  n.update=5000
  n.iter= 5000
jm.pooled = jags.model(file="PooledJAGS.R", data=data, n.adapt = n.adapt, inits=inits, n.chains=length(inits))


```



```{r, include=key, echo=key, eval=key}
update(jm.pooled, n.iter = n.update)
zc.pooled = coda.samples(jm.pooled, variable.names = c("alpha", "beta", "sigma"), n.iter=n.iter)
MCMCplot(zc.pooled)
MCMCtrace(zc.pooled)
MCMCsummary(zc.pooled, n.eff=TRUE)
```

#### V. Intercepts for each site
Now you will implement the model that allows intercept to vary by group, where each intercept is drawn from a common distribution. Again, use the template provided below to allow you to concentrate on writing JAGS code for the model. Note that you must use the index trick covered in lecture to align the different groups with different intercepts. Here are the preliminaries to set up the model:

```{r}
x_for_plotting = seq(25,1250,10)
data = list(
  y.emission = log(y$emission),
  y.n.input = log(y$n.input) - mean(log(y$n.input)), #center the data to speed convergence and aid in interpretation. Can recover 0 intercept if needed.
  y.group = y$group.index,  
  y.n.sites = length(unique(y$group)),
  x.hat = log(x_for_plotting) - mean(log(y$n.input))
)

inits = list(
  list(
    alpha = rep(0,y.n.sites),
    beta = .5,
    sigma = 50,
    mu.alpha= 0,
    sigma.alpha = 10
  ),
  list(
    alpha = rep(1,y.n.sites),
    beta = 1.5,
    sigma = 10,
    mu.alpha= -2,
    sigma.alpha = 20
  )
)

```

Whenever I write code for a multi-level model like this one I do it incrementally, starting with a separate model for each site.  I then do the partial pooling, drawing the intercept for each model from a distribution.  I do this because it is always best to do the simple thing first: there is less to go wrong. I add the pooling of intercepts after the individual model approach is working.  Implementing the two types of models requires modifying only four lines of code.  I suggest you use this incremental approach, commenting out the code for a model for each site when you go to the random intercept (aka partially pooled) model.

Write the model code. Compile the model and summarize coda output. Test for convergence.

**Extra Credit**: Plot the mean and median of model predictions of emissions and .95 highest posterior density intervals as a function of fertilizer additions across all possible sites including sites that were not observed. This is a new, unvetted, "figure it out" problem.  I included in the folder for this lab a description of math from a related problem from my own research and some example code to illustrate how to do it.  Be sure you understand what is going on. You may assume for this problem that the inclusion of sites in data was completely random. 


You will need lots of iterations (100,000 ??) to get smooth credible intervals. Get the plot approximately right using 25,000 iterations, then increase them as needed for the final plot.

```{r, include=key, echo=key, eval=key}
{ #note this opening { and the closing } are needed by R markdown but not by R
####Hierarchical model, site level intercept, no site covariate
sink("Hier_1")
cat("
    model{
    ##hyperpriors
    #comment out for model for each sites######
    mu.alpha ~ dnorm(0,.00001)  
    sigma.alpha ~ dunif(0,200) #notated varsigma in model documentation
    tau.alpha <- 1/sigma.alpha^2
    ##############
    sigma ~ dunif(0,100)
    tau.reg <- 1/sigma^2
    ###priors
    for(j in 1:y.n.sites){
        alpha[j] ~ dnorm(mu.alpha,tau.alpha)  #for multi-level model
        #alpha[j] ~ dnorm(0,.000001) #unpooled model for each site
      }
    beta ~ dnorm(0,.0001)
    ####
    #likelihood
    #note that data have been log-transformed on the R side. 
    for(i in 1:length(y.emission)){
        log_mu[i] <- alpha[y.group[i]] + beta * y.n.input[i]
        y.emission[i] ~ dnorm(log_mu[i], tau.reg)
    } #end of i
 #prediction of mean emissions from all possible sites
  for(i in 1:length(x.hat)){
    alpha.new[i] ~ dnorm(mu.alpha,tau.alpha)
    #prediction of mean at site j
    log_mu.all[i] <- alpha.new[i] + beta * x.hat[i]
    mu.hat.all[i] <- exp(log_mu.all[i]) 
  } #end of i
    
    } #end of model
    
    ",fill=TRUE)
sink()
}
```



```{r, include=key, echo=key, eval=key, fig.height=8}
n.update=10000
n.iter=25000

jm.hier1 = jags.model("Hier_1", data=data, n.adapt = 3000, inits=inits, n.chains=length(inits))
update(jm.hier1, n.iter = n.update)
#You would want to include alphas in check for convergnce but I eliminated them here to make output more compact.
zc.hier1 = coda.samples(jm.hier1, variable.names = c("sigma","beta", "mu.alpha", "sigma.alpha", "mu.hat.all", "y.new.all"), n.iter=n.iter)
MCMCplot(zc.hier1, params=c(c("sigma","beta", "mu.alpha", "sigma.alpha")))
MCMCtrace(zc.hier1,params=c("sigma","beta", "mu.alpha", "sigma.alpha"))
MCMCsummary(zc.hier1, n.eff=TRUE, params=c("sigma", "beta", "mu.alpha", "sigma.alpha"))

mean_NO2 = MCMCpstr(zc.hier1, params=c("mu.hat.all"), func = mean)
med_NO2 = MCMCpstr(zc.hier1, params=c("mu.hat.all"), func = median)

HPDI <-  MCMCpstr(zc.hier1, params=c("mu.hat.all"), func = function(x) hdi(x,.95))

par(mfrow=c(1,1))
plot(y$n.input,y$emission, pch=19, cex=.1, main = "Prediction of mean emissions across all possible sites", xlab="Nitrogen inputs (kg / ha / year)", ylab = "Nitrous oxide emissions (g / ha / day)")
lines(x_for_plotting,mean_NO2$mu.hat.all, type="l", col = "red")
lines(x_for_plotting,med_NO2$mu.hat.all, type="l", col = "red", lty="dashed")
lines(x_for_plotting, HPDI$mu.hat.all[,1], col = "blue", lty="dotted") 
lines(x_for_plotting, HPDI$mu.hat.all[,2],  col = "blue", lty="dotted")  
```
#### VI. Intercepts vary with carbon level in site soils 

Modify your model to include a covariate at the site level, soil carbon content,  as developed in the model writing problem #3.

Set up data and initial conditions:

```{r}
#######Hierarchical model, site level intercept predicted from carbon concentration covariate and slope varying with fertilizer type. 
x_for_plotting = seq(26,1250,10)
data = list(
  y.emission = log(y$emission),
  y.n.input = log(y$n.input)-mean(log(y$n.input)), #center the data to speed convergence and aid in interpretation
  y.group=  y$group.index,
  y.n.sites = length(unique(y$group)),
  w = log(w$mean/(1-w$mean)),   #logit of w$mean
  x.hat = log(x_for_plotting) - mean(log(y$n.input))
)
y.n.sites = length(unique(y$group))
y.n.fert = length(unique(y$fertilizer))
inits = list(
  list(
    alpha = rep(0,y.n.sites),
    beta = .5,
    sigma = 50,
    sigma.alpha = 10,
    eta = .2,
    kappa = .5
  ),
  list(
    alpha = rep(-.2,y.n.sites),
    beta = 1.5,
    sigma = 10,
    sigma.alpha = 20,
    eta = .2,
    kappa = 5
  )
)

```

Write the model code and compile it.  Summarize the coda object in a table excluding $\alpha$. Produce a caterpillar plot for $\alpha$ excluding the label on the y axis and ranking in descending order.  Produce trace plots for $\eta$, $\kappa$, and $\beta$.

**Extra credit**: Plot the mean and median of model predictions of emissions and .95 highest posterior density intervals as a function of fertilizer additions across the specific sites studied. This is a "figure it out" problem.  I included in the folder for this lab a description of math from a related problem from my own research and some example code to illustrate how to do it.  Be sure you understand what is going on.  You may assume for this problem that the inclusion of sites in the data was completely random. 

```{r, include=key, echo=key, eval=key}
{
sink("Hier_2")
cat("
    model{
    #priors for within site model######
    sigma ~ dunif(0,200)
    tau.reg <- 1/sigma^2
    beta ~ dnorm(0,.00001)
    #priors for intercept model#######
    kappa ~ dnorm(0,.00001)
    eta ~ dnorm(0, .000001)
    sigma.alpha ~ dunif(0,200)
    tau.alpha <- 1/sigma.alpha^2
    
  

    #likelihood for data, note that data are on log scale in data statement on R side.  
    for(i in 1:length(y.emission)){
      log_mu[i] <- alpha[y.group[i]] + beta * y.n.input[i]
      y.emission[i] ~ dnorm(log_mu[i], tau.reg)
    }
    # carbon model for intercept
  for(j in 1:y.n.sites){
     #use normal because data are centered
      mu.alpha[j] <- kappa + eta *w[j]
      alpha[j] ~ dnorm(mu.alpha[j],tau.alpha)
  }
  #set up discrete uniform using dcat
   for(j in 1:y.n.sites){
    group[j] <- 1
  }
  p <- group[]/sum(group)
 for(i in 1:length(x.hat)){
   j[i] ~ dcat(p[])
   #prediction of mean at site j[i] on exponentiated scale
   log_mu.fixed[i] <- alpha[j[i]] + beta * x.hat[i]
   mu.hat.fixed[i] <- exp(log_mu.fixed[i]) 
    } #end of i
  
 } #end of model
    
    ",fill=TRUE)
sink()
  
}
```



```{r, include=key, echo=key, eval=key}
n.update=25000
n.iter=25000

jm.hier2 = jags.model("Hier_2", data=data, n.adapt = 3000, inits=inits, n.chains=length(inits))

update(jm.hier2, n.iter = n.update)
#You would want to include alphas in check for convergnce but I eliminated them here to make output more compact.
zc.hier2 = coda.samples(jm.hier2, variable.names = c("sigma","eta", "kappa", "alpha", "beta", "mu.hat.fixed", "j"), n.iter=n.iter)
MCMCplot(zc.hier2, params="alpha", labels = NULL, rank = TRUE)
MCMCtrace(zc.hier2, params = c("eta", "kappa", "beta") )
MCMCsummary(zc.hier2, excl="alpha", n.eff=TRUE)

mean_NO2 = MCMCpstr(zc.hier2, params=c("mu.hat.fixed"), func = mean)

med_NO2 = MCMCpstr(zc.hier2, params=c("mu.hat.fixed"), func = median)

HPDI <-  MCMCpstr(zc.hier2, params=c("mu.hat.fixed"), func = function(x) hdi(x,.95))

par(mfrow=c(1,1))
plot(y$n.input,y$emission, pch=19, cex=.1, main = "Prediction of mean emissions for the sites studied", xlab="Nitrogen inputs (kg / ha / year)", ylab = "Nitrous oxide emissions (g / ha / day)")
lines(x_for_plotting,mean_NO2$mu.hat.fixed, type="l", col = "red")
lines(x_for_plotting,med_NO2$mu.hat.fixed, type="l", col = "red", lty="dashed")
lines(x_for_plotting, HPDI$mu.hat.fixed[,1], col = "blue", lty="dotted") 
lines(x_for_plotting, HPDI$mu.hat.fixed[,2],  col = "blue", lty="dotted")  


```
####VII. Intercepts vary with carbon level in site soils and slopes vary with fertilizer type

Modify your model with the covariate at the site level, soil carbon content, to allow slopes to vary with fertilizer type as developed in the model writing problem #4.

Set up data and initial conditions:


```{r}
#######Hierarchical model, site level intercept predicted from carbon concentration covariate and slope varying with fertilizer type. 

data = list(
  y.emission = log(y$emission),
  y.n.input = log(y$n.input)-mean(log(y$n.input)), #center the data to speed convergence and aid in interpretation
  y.group=  y$group.index,
  y.fert = y$fert.index,
  y.n.sites = length(unique(y$group)),
  y.n.fert = length(unique(y$fertilizer)),
  w = log(w$mean/(1-w$mean))   #logit of w$mean
)
y.n.sites = length(unique(y$group))
y.n.fert = length(unique(y$fertilizer))
inits = list(
  list(
    alpha = rep(0,y.n.sites),
    beta = rep(.5,y.n.fert),
    sigma = 50,
    sigma.alpha = 10,
    eta = .2,
    kappa = .5
  ),
  list(
    alpha = rep(-.2,y.n.sites),
    beta = rep(1.5, y.n.fert),
    sigma = 10,
    sigma.alpha = 20,
    eta = .2,
    kappa = 5
  )
)

```

Write the model code and compile it. Produce a CODA object.  Summarize the coda object and test for converge.  Examine the effective sample size. Are there any parameters that need more iterations?

Prepare a caterpillar plot of the slopes. 

Plot a *histogram* approximating the the marginal posterior of $\mu_\beta$. (Do not us MCMCtrace.)




```{r, include=key, echo=key, eval=key}
{
sink("Hier_3")
cat("
    model{
    #priors for within site model######
    sigma ~ dunif(0,200)
    tau.reg <- 1/sigma^2
    
    #priors for intercept model#######
    kappa ~ dnorm(0,.00001)
    eta ~ dnorm(0, .000001)
    sigma.alpha ~ dunif(0,200)
    tau.alpha <- 1/sigma.alpha^2
    #hyper priors for slope model
    mu.beta ~ dnorm(0,.00001)
    sigma.beta ~ dunif(0,200)
    tau.beta <- 1/sigma.beta
  

    #likelihood for data, note that data are on log scale in data statement on R side.  
    for(i in 1:length(y.emission)){
      log_mu[i] <- alpha[y.group[i]] + beta[y.fert[i]] * y.n.input[i]
      y.emission[i] ~ dnorm(log_mu[i], tau.reg)
    }
    # carbon model for intercept
  for(j in 1:y.n.sites){
     #use normal because data are centered
      mu.alpha[j] <- kappa + eta *w[j]
      alpha[j] ~ dnorm(mu.alpha[j],tau.alpha)
  }
  #Allow slope to vary by fertilizer type
  for(k in 1:y.n.fert){
    beta[k] ~ dnorm(mu.beta, tau.beta)
  }
 } #end of model
    
    ",fill=TRUE)
sink()
  
}
```



```{r, include=key, echo=key, eval=key, fig.height=15}
n.update=50000
n.iter=25000

jm.hier3 = jags.model("Hier_3", data=data, n.adapt = 3000, inits=inits, n.chains=length(inits))

update(jm.hier3, n.iter = n.update)
zc.hier3 = coda.samples(jm.hier3, variable.names = c("sigma","eta", "kappa","beta", "alpha", "mu.beta", "sigma.beta"), n.iter=n.iter)
#zj.hier3 = jags.samples(jm.hier3, variable.names = c(variable.names = c("alpha", "beta", "sigma","eta", "kappa")), n.iter=n.iter)
MCMCplot(zc.hier3, params = "alpha", rank = TRUE, labels = NULL)
MCMCplot(zc.hier3, params = "beta")
MCMCsummary(zc.hier3, excl = "alpha", n.eff=TRUE)
hist(MCMCchains(zc.hier3,params = "mu.beta"), breaks = 100, main = "", freq=FALSE,xlab =expression(mu[beta]))



```

#### IX. Slope *and* intercepts vary by site
We now want to allow *both* slopes and intercepts to vary by site as described in the math exercise. This is a pretty challenging coding problem, so I thought it would be more useful for you to study code that works than to try to write the code yourself.  So take a careful look at the following and discuss it with your lab mates. Are the slope and intercepts correlated? How could you make a probabilistic statement about the correlation?

As usual, we set up data and initial values:

```{r}
data = list(
  y.emission = log(y$emission),
  y.n.input = log(y$n.input)-mean(log(y$n.input)), #center the data to speed convergence and aid in interpretation-- there is no such thing as soil with 0 carbon
  y.group=  y$group.index,
   y.n.sites = length(unique(y$group)),
  y.n.fert = length(unique(y$fertilizer))
)
y.n.sites = length(unique(y$group))
B = matrix(nrow=y.n.sites, ncol=2)
B[,1]=0
B[,2]=1.5
inits = list(
  list(
    B=B,
    sigma = 50,
    mu.alpha = 0,
    mu.beta = 1.5,
    sigma.alpha = 10,
    sigma.beta = 10,
    rho=-.5
  ),
  list(
    B=B*.5,
    sigma = 20,
    mu.alpha = -.2,
    mu.beta = .8,
    sigma.alpha = 50,
    sigma.beta = 50,
    rho=.5
  )
)


```

Here is the code for the model. Study it relative to the math.
```{r}
{
sink("Hier_4")
cat("
    model{
    #priors for within site model######
    sigma ~ dunif(0,200)
    tau.reg <- 1/sigma^2
    
    #likelihood for data, note that data are on log scale.
    for(i in 1:length(y.emission)){
      log_mu[i] <- alpha[y.group[i]] + beta[y.group[i]] * y.n.input[i]
      y.emission[i] ~ dnorm(log_mu[i], tau.reg)
    }
    # Model for group intercept and slope:
    for(j in 1:y.n.sites){
        alpha[j] <- B[j,1]  #group level intercept
        beta[j]  <- B[j,2]  #group level slope
        B[j,1:2] ~ dmnorm(B.hat[j,1:2], Tau.B)  
        B.hat[j,1] <- mu.alpha  #required by JAGS syntax
        B.hat[j,2] <- mu.beta   #required by JAGS syntax
    }
    mu.alpha ~ dnorm(0,.0001)  #mean intercept
    mu.beta ~ dnorm(0, .0001)  #mean slope
    #Inverse of covariance matrix required by JAGS
    Tau.B[1:2,1:2] <- inverse(Sigma.B[1:2,1:2])
    #Elements of covariance matrix
    Sigma.B[1,1] <- sigma.alpha^2
    sigma.alpha ~ dunif(0,200)
    Sigma.B[2,2] <- sigma.beta^2
    sigma.beta ~ dunif(0,200)
    Sigma.B[1,2] <- rho*sigma.alpha*sigma.beta  # covariance is correlation coef. x product of standard deviations
    Sigma.B[2,1] <- Sigma.B[1,2]
    rho ~ dunif(-1,1)
    } #end of model
    
    ",fill=TRUE)
sink()
}
```

Compile the model and get some output.  

```{r}
n.update=25000
n.iter=10000

jm.hier4 = jags.model("Hier_4", data=data, n.adapt = 3000, inits=inits, n.chains=length(inits))


update(jm.hier4, n.iter = n.update)
#You should run diagnostics on zc.hier coda object for intecepts and slopes but I eliminated them here to make output more compact
zc.hier4 = coda.samples(jm.hier4, variable.names = c('mu.alpha', "mu.beta", "rho", "beta", "alpha"), n.iter=n.iter)
MCMCsummary(zc.hier4, excl="alpha", n.eff=TRUE)
MCMCplot(zc.hier4, params = "beta", rank=TRUE, labels=NULL, xlab =expression(paste("Inference on ", beta)))


```

#### X.  Model checking

#### Motivation
All statistical inference is based on some type of statistical model.  A truly fundamental requirement for reliable inference is that the statistical model is capable of giving rise to the data.  If this requirement is not met, you have *no* basis for inference. Statistical theory will not back you up. You are flying blind, proceeding bravely, perhaps foolishly, on your own. These truths motivate the need for model checking.  Models that fail checks should be discarded at the outset.  (This is *not* model selection.  More about that later.)

The Bayesian approach provides a method, simple to implement, that allows you to check if your model is capable of producing the  data.  It is called a *posterior predictive check*.  The details of the math are given Hobbs and Hooten 8.1 and were covered in lecture. Here is a brief description of how to code it.  The algorithm goes like this:

1. Simulate a new data set at each iteration of the MCMC.  This sounds formidable, but it is really no more than drawing a random variable from the likelihood.  So, for example if your likelihood is 
```{r, eval=FALSE}
y[i] ~ dnorm(log_mu[i], tau)
```
you can simulate a new data set by embedding  

```{r, eval = FALSE}
y.sim[i] ~ dnorm(log_mu[i], tau)
```

in the same `for` loop.

2. Calculate a test statistic based on the real and the simulated data.  The test statistic could be a mean, standard deviation, coefficient of variation, discrepancy, minimum, maximum -- really any function that helps you compare the simulated and real data. I think it is wise to always include a test statistic that in someway reflects the variance because it is harder for the model to get the variance 

3. We are interested in calculating a Bayesian p value, the probability that the test statistic computed from the simulated data is more extreme than the test statistic computed from the real data. There is evidence of lack of fit -- the model cannot give rise to the data -- if the Bayesian p value is large or small.  We want values between, say, .10 and .90, ideally close to .5. To obtain this the Bayesian p we use the JAGS `step(x)` function that returns 0 if x is less 0 and and 1 otherwise.  So, presume our test statistic for was the standard deviation.  Consider the following pseudo-code:
```{r eval=FALSE}
for(i in 1:length(y)){
  mu[i] <- prediction from model
  y[i] ~ dnorm(mu[i], tau)
  y.sim[i] ~ dnorm(mu[i], tau)
}
sd.data<-sd(y[])
sd.sim <-sd(y.sim[])
p.sd <- step(sd.sim - sd.data)
```
That is all there is to it.  You then include `p.sd` in your jags or coda object.

###Problem

Return to the pooled model you developed in the first problem of multi-level modeling exercise. Do posterior predictive checks using the mean, standard deviation, minimum, and discrepancy as test statistics.  The discrepancy statistic is is $\sum_{i=1}^{n}(y_i-\mu_i)^2$ where $\mu_i$ is the $i^th$ prediction of your model. Overlay the posterior distribution of the simulated data on the histogram of the read data (density on y axis, not frequency). What do you conclude?  Is there any indication of lack of fit?  Enough to discard the model?

```{r}
#preliminaries
library(rjags)
library(reshape)
library(ggplot2)
set.seed(5)
#setwd("/Users/Tom/Documents/Ecological Modeling Course/_A_Master_Lab_Exercises/Multi-level models NO2/")
y.n.sites = length(unique(y$group))

#data for all models except last one
data = list(
  y.emission = log(y$emission),
  y.n.input = log(y$n.input) - mean(log(y$n.input)) #center the data to speed convergence and aid in interpretation. Can recover 0 intercept if needed.

)



inits = list(
  list(
    alpha = 0,
    beta = .5,
    sigma = 50
  ),
  list(
    alpha = 1,
    beta = 1.5,
    sigma = 10
  )
)


```



```{r,eval=key, echo=key}
####Pooled model
{
sink("Pooled")
cat("
model{
#priors
alpha ~ dnorm(0,.0001)
beta ~ dnorm(0,.0001)
sigma ~ dunif(0,100)
tau.reg <- 1/sigma^2
#likelihood
 for(i in 1:length(y.emission)){
    mu[i] <- alpha + beta * y.n.input[i]
    y.emission[i] ~ dnorm(mu[i], tau.reg)
    #simulated data for posterior predictive checks
    y.emission.sim[i] ~ dnorm(mu[i], tau.reg) 
    sq.error.data[i] <- (y.emission[i]-mu[i])^2
    sq.error.sim[i] <- (y.emission.sim[i] - mu[i])^2
 }
#Bayesian P values
sd.data <- sd(y.emission)
sd.sim <- sd(y.emission.sim)
p.sd <- step(sd.sim-sd.data)

mean.data <- mean(y.emission)
mean.sim  <- mean(y.emission.sim)
p.mean <- step(mean.sim - mean.data)

discrep.data <- sum(sq.error.data)
discrep.sim <- sum(sq.error.sim)
p.discrep <- step(discrep.sim - discrep.data)

min.data <- min(y.emission)
min.sim <- min(y.emission.sim)
p.min <-step(min.sim-min.data)
}
    
",fill=TRUE)
sink()
}
```




```{r, eval=key, echo=key}
n.update=10000
n.iter=10000
n.update = 3000
jm.pooled = jags.model("Pooled", data=data, n.adapt = 3000, inits=inits, n.chains=length(inits))
update(jm.pooled, n.iter = n.update)
zc.pooled = coda.samples(jm.pooled, variable.names = c("alpha", "beta", "sigma", "p.sd", "p.mean", "p.discrep","p.min", "y.emission.sim"), n.iter=n.iter)
#zj.pooled = jags.samples(jm.pooled, variable.names = c("alpha", "beta", "sigma", "p.sd", "p.mean", "p.discrep","p.min", "y.emission.sim"), n.iter=n.iter)


MCMCsummary(zc.pooled, params = c("p.sd", "p.mean", "p.discrep","p.min"))

hist(data$y.emission, breaks=20, freq=FALSE, main="Simulated and real data", xlab=expression(paste("log(", N[2], "0 emission)")), cex.lab=1.2) #note that this is the log transformed data
lines(density(MCMCchains(zc.pooled,params="y.emission.sim")), col="red")
legend(-10,.2,"simulated", col="red", lty="solid")

```

