---
title: "Rules of Probability"
author: "Chris Che-Castaldo, Mary B. Collins, N. Thompson Hobbs"
date: "August 2017"
output: 
  beamer_presentation:
    includes:
      in_header: header.tex
theme: Boadilla
subtitle: Bayesian Modeling for Socio-Environmental Data
latex_engine: xelatex
transition: fastest
---

## Road map

- Rules of probability
    + Conditional probability
    + Independence
    + The law of total probability
- Factoring joint probabilities

# Rules of probability

## Why do we need to know this stuff?

1. **Conditional probability** foundational for all the inferences that we make.
2. **The law of total probability** is the denominator of Bayes' Theorem.
3. **Factoring** joint distributions is how we deal with complexity, reducing high dimensional problems.
4. **Independence** allows us to simplify fully factored joint distributions.
5. Bayesian inference is based on **marginal distributions** of unobserved quantities.

## Random variables:

- are quantities governed by chance.
- have a specific value called an _events_ or _outcomes_.
- are summarized by probability distributions. 

Bayesians treat every unobserved quantity as a random variable.

## S=Sample Space

- The set of all possible events or outcomes of an experiment or survey.
- The sample space, $S$ has a specific area.

\center\includegraphics[width=0.95\paperwidth]{../../Graphics/ConceptsTheory.png}

\includegraphics[height=1.25in]{S.png}

## Events in S

- Can define and event, $A$.
- The area of event $A$ is less than $S$.

\includegraphics[height=1.25in]{eventA.png}

## What is the probability of event A?

\includegraphics[height=1.25in]{eventA.png}

$\Pr(A) = \frac{\text{Area of}~A}{\text{Area of}~S}$

<!-- ## What is the probability of event A? -->

<!-- - Area of A = 4 -->
<!-- - Area of S = 20 -->

<!-- \includegraphics[height=1.25in]{eventA.png} -->

<!-- $\Pr(A) = \frac{\text{Area A}}{\text{Area S}} = \frac{4}{20} = .2$ -->

## Conditional Probability
*Conditional probability*: the probability of an event given that _we know_ another event has occurred.

\includegraphics[height=1.25in]{eventB.png}


## What is the probability of event $B$, given that event $A$ has occurred?

<!-- - Area S = 20 -->
<!-- - Area A = 4 -->
<!-- - Area B = 2 -->
<!-- - Area A $\cap$ B =1 -->

\includegraphics[height=1.25in]{eventB.png}

$\Pr(B|A)$ = probability of $B$ conditional on knowing $A$ has occurred



## What is the probability of event $B$, given that event $A$ has occurred?

<!-- - Area A = 4 -->
<!-- - Area B = 2 -->
<!-- - Area A $\cap$ B =1 -->
<!-- - Area S = 20 -->

\includegraphics[height=1.25in]{eventB.png}

$Pr(B|A) = \frac{\text{Joint Probability}}{\text{Probability of A}}=\frac{\Pr(A,B)} {\Pr(A)}$

## What about the probability of event $B$, given that two other events have occured, events $A$ and $C$?

$Pr(B|A,C) = \frac{\Pr(A,B,C)} {\Pr(A)}$



## If the occurance of event A does not tell us anything about event B?

_In this case, events A and B are said to be **independent**_

## Events are independent if and only if...

$\Pr(A|B) = \Pr(A)$

<!-- ##  -->

<!-- \includegraphics[height=3in]{rect3823.png} -->

## Assuming independence, the joint probability of event A and event B

<!-- - Area A = 4 -->
<!-- - Area B = 3 -->
<!-- - Area A $\cap$ B =.6 -->
<!-- - Area S = 20 -->

\includegraphics[height=2.25in]{rect3823.png}

$\Pr(A,B) = \Pr(A) \Pr(B)$

<!-- = \Big(\frac{4}{20}\Big) \Big(\frac{3}{20}\Big)=0.03$ -->





## The Law of Total Probability

\includegraphics[height=1.25in]{totalProb.png}

We can define a set of events $\{B_n : n = 1,2,3,...\}$, which taken together define the entire sample space, $\sum_n B_n = S$.

## What is the probability of event A?

\includegraphics[height=1.25in]{totalProb.png}

$\Pr(A) = \sum_n \Pr(A|B_n)\Pr(B_n)$ (discrete case)

$\Pr(A) = \int \Pr(A|B)\Pr(B) dB$ (continuous case)




## The Chain Rule of Probability 
The chain rule of probability allows us to calculate any number of joint distributions using only conditional probabilities.
\vspace{1cm}


$\Pr(z_1,z_2,...,z_n) = \Pr(z_n|z_{n-1},...,z_1) ... \Pr(z_3|z_2,z_1)\Pr(z_2|z_1)\Pr(z_1)$


\vspace{1cm}
Notice the pattern here.

- z’s can be scalars or vectors.
- Sequence of conditioning doesn’t matter.
- When we build models, we choose a sequence that makes sense.

##
Chain rule of probability board work and independence

## Factoring joint probabilities

Why is factoring useful?

- The rules of probability allow us to simplify complicated joint distributions, breaking them down into chunks.
- Chunks can be analyzed one at a time.
- Provide a usable graphical and mathematical foundation, _critical_ for the model specification step.

## Consider a Bayesian Network (represented by a directed acyclic graph or DAG)

\centerline{\includegraphics[height=.8in]{factoringI}}

 - Bayesian networks specify how joint distributions are factored into conditional distributions using nodes to represent RV's and arrows to represent dependencies.
- Nodes at the heads of arrows _must_ be on the left hand side of the conditioning symbols;
- Nodes at the tails of arrows are on the right hand side of the conditioning symbols.
- Any node at the tail of an arrow without and arrow leading into it must be expressed unconditionally.

## Factoring with DAGs

\centerline{\includegraphics[height=.8in]{factoringI}}

$\Pr(A,B) =$

## Factoring with DAGs

\centerline{\includegraphics[height=.8in]{factoringI}}

$\Pr(A,B) = \Pr(A|B) \Pr(B)$

## Factoring with DAGs

\centerline{\includegraphics[height=.8in]{abcDag}}

$\Pr(A,B,C) =$

## Factoring with DAGs

\centerline{\includegraphics[height=.8in]{abcDag}}

$\Pr(A,B,C) = \Pr(A|B,C)\Pr(B|C)\Pr(C)$

<!-- ## Generalizing -->

<!-- $\Pr(z_1,...,z_n)=\prod^n_{i=1}\Pr(z_i|\left\{P_i\right\})$ -->

<!-- \vspace{1cm} -->

<!-- $\left\{P_i\right\}$ is the set of parents of node $z_i$ -->

## Work on lab
Complete parts I-VI

<!-- ## Marginal distributions -->

<!-- The marginal distribution of a subset of a collection of random variables is the probability distribution of the random variables contained in the subset. -->

<!-- \centerline{\includegraphics[height=2in]{marginalWiki}} -->

<!-- ## Marginal Distributions Example -->

<!-- Consider two discrete random variables that are jointly distributed such that,  -->

<!-- $[x,y] \equiv \Pr(x,y)$ -->

<!-- ## Marginal Distributions Example -->
<!-- We are studying a species for which births occur in pulses.  We observe 100 females and record the age of each animal and the number of offspring produced.  -->

<!-- \centerline{\includegraphics[height=1.25in]{marginal}} -->

<!-- When we have a joint distribution of two random variables, we can focus on one by summing over the probabilities of the other. -->

<!-- ## Marginal Distributions Example -->

<!-- - We care about marginal distributions because they allow us to represent the single variable distribution of unknown quantities that are parts of joint distributions. -->
<!-- - They are a vital tool for simplification. -->

<!-- ## Work on lab -->
<!-- Complete part VII -->

