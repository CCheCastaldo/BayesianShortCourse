\documentclass[ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Boadilla}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography
\hypersetup{
            pdftitle={Likelihood},
            pdfauthor={Chris Che-Castaldo, Mary B. Collins, N. Thompson Hobbs},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \let\insertsectionnumber\relax
    \let\sectionname\relax
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathptmx}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{pgfpages}
\usepackage[mathscr]{euscript}
 \usepackage{tikz}

%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=right}

\setbeamersize{text margin left=10pt, text margin right=20pt}

\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}Che-Castaldo, Collins, Hobbs
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.6\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle, \insertdate
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.1\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\makeatletter
\setbeamertemplate{navigation symbols}{}

\titlegraphic{\includegraphics[width=6cm]{../../Headers/Logo.png}}

\title{Likelihood}
\subtitle{Bayesian Modeling for Socio-Environmental Data}
\author{Chris Che-Castaldo, Mary B. Collins, N. Thompson Hobbs}
\date{June 2019}

\begin{document}
\frame{\titlepage}

\begin{frame}{Why Likelihood?}

\begin{itemize}
\tightlist
\item
  Likelihood is a component of all Bayesian models.
\item
  Maximum likelihood is an important statistical approach in its own
  right.
\end{itemize}

\end{frame}

\begin{frame}{Learning objectives for lecture and lab}

\begin{itemize}
\tightlist
\item
  Understand the concept of likelihood and its relationship to the
  probability of data conditional on parameters.
\item
  Describe a likelihood profile and how it differs from a probability
  density function.
\item
  Understand how to compose likelihoods for multiple parameters and
  multiple observations.
\end{itemize}

\end{frame}

\begin{frame}{Data conditional on a parameter: \([\, y \mid \theta]\)}

Prevalence is a term used in disease ecology to indicate the proportion
of a population that is infected. The true prevalence, \(\theta\), of
chronic wasting disease in male mule deer on the winter range north of
Fort Collins is 12\%. A sample of 24 male deer includes 4 infected
individuals. What is the probability of obtaining these data if the
estimate of prevalence is true?

\end{frame}

\begin{frame}{Parameter conditional the data: \([\theta \mid y]\)}

We obtain a new sample of 24 male mule deer on the winter range north of
Fort Collins that includes 4 infected individuals. In light of these
data, what is the probability that the true value of prevalence,
\(\theta\), is found in \(q_{L} \le \theta \le q_{U}\)?

\end{frame}

\begin{frame}{Bayesian inference is based on \([\theta \mid y]\)}

\begin{center}
\input{tempPlot1.tex}
\end{center}

\end{frame}

\begin{frame}{Inference from likelihood is based on
\([\, y \mid \theta]\)}

\begin{center}
\input{tempPlot2.tex}
\end{center}

Likelihood allows us to compare alternative parameter values and models
by calculating the probability of the data conditional on the parameters
\([\, y \mid \theta]\). As you will see, all evidence based on
likelihood is relative.

\end{frame}

\begin{frame}{The key idea in likelihood}

\begin{itemize}
\tightlist
\item
  In a probability mass or probability density function, the parameter
  \(\theta\) is constant (known) and the data \(\bm{y}\) are random
  variables. The function sums or integrates to 1 over its support.
\item
  In a likelihood function, the data are constant (known) and the
  parameter is unknown but fixed. We use \([\, y \mid \theta]\) to
  assess the likelihood of different values of \(\theta\) in light of
  the data. In this case, the function does not sum or integrate to one
  over all possible values of the parameter.
\end{itemize}

\[
\underbrace{L\left({\theta \mid y}\right)}_{{\text{likelihood function}}} \propto \underbrace{[\, y \mid \theta]}_{\text{PDF or PMF}}
\] \center Likelihood is \emph{proportional} to probability or
probability density

\end{frame}

\begin{frame}{Discuss notation}

\begin{eqnarray}
L(\theta \mid y) & \propto & [\, y \mid \theta]\\
L(\theta \mid y) & = & c[\, y \mid \theta]\\
L(\theta \mid y) & = & [\, y \mid \theta]
\end{eqnarray}

\end{frame}

\begin{frame}{The parameter is known and the data are random}

\vspace*{1cm}

\centerline{\includegraphics[height=5cm]{../../Graphics/CanBeans.png}}

\begin{itemize}
\tightlist
\item
  What are the possible outcomes?
\item
  What probability mass function would you use to model theses data?
\item
  What is the probability of each outcome?
\item
  What is the sum of the individual probabilities?
\end{itemize}

\end{frame}

\begin{frame}{The parameter is known and the data are random}

We draw three beans from a can with equal numbers of white and black
beans. The possible outcomes are:

\bigskip \center

\begin{tabular}{|c|c|c|}
\hline 

$\theta$ & number of white beans, $y_{i}$ & $[\, y_{i} \mid \theta=.5]$\tabularnewline
\hline 
\hline 
.5 & 0 & .125\tabularnewline
\hline 
.5 & 1 & .375\tabularnewline
\hline 
.5 & 2 & .375\tabularnewline
\hline 
.5 & 3 & .125\tabularnewline
\hline 
& $\sum_{i=1}^{4}[\, y_{i} \mid \theta=.5]$ & 1
\tabularnewline
\hline 
\end{tabular}

\end{frame}

\begin{frame}{The data are known and the parameter is random}

\vspace*{1cm}

\centerline{\includegraphics[height=6.2cm]{../../Graphics/CansBeans.png}}

\begin{itemize}
\tightlist
\item
  What is the likelihood of each parameter value?
\end{itemize}

\end{frame}

\begin{frame}{The data are known and the parameter is random}

We have three hypothesized parameter values,
\((\nicefrac{5}{6}, \nicefrac{1}{2}, \nicefrac{1}{6})\). Data in hand
are 2 white beans on three draws. The likelihood of each parameter value
is:

\bigskip \center

\begin{tabular}{|c|c|c|}
\hline 
hypothesis, $\theta_{i}$ & number of white beans, $y$ & $[\, y=2 \mid \theta_{i}]$\tabularnewline
\hline 
\hline 
$\theta_{1}=\nicefrac{5}{6}$ & 2 & 0.35\tabularnewline
\hline 
$\theta_{2}=\nicefrac{1}{2}$ & 2 & 0.38\tabularnewline
\hline 
$\theta_{3}=\nicefrac{1}{6}$ &  2 & 0.069\tabularnewline
\hline 
& $\sum_{i=1}^{3}[\, y=2 \mid \theta_{i}]$ & 0.79\tabularnewline
\hline 
\end{tabular}

\end{frame}

\begin{frame}{A likelihood profile: 2 white beans on 3 draws}

\begin{center}
\input{tempPlot3.tex}
\end{center}

\end{frame}

\begin{frame}{Class exercise: Can of beans}

Have someone take a draw of 10 beans from one of the cans where the
identity of the can is unknown.

\begin{itemize}
\tightlist
\item
  Which of the three cans is the most likely to have produced this draw?
\item
  How much more likely is this can than the other two?
\end{itemize}

\end{frame}

\begin{frame}{A likelihood profile: 4 white beans on 10 draws}

\begin{center}
\input{tempPlot4.tex}
\end{center}

\end{frame}

\begin{frame}{Likelihood vs Probability:}

\begin{center}
\input{tempPlot5.tex}
\end{center}

\end{frame}

\begin{frame}{How do we fit models with multiple parameters?}

In the example we had a single parameter, \(\theta\), one set of
observation, 4 successes on 10 draws, and a binomial likelihood.
However, we could have made the likelihood a function of the
\emph{predictions} of a model, and used any probability mass function or
probability density function as a ``wrapper'' for the predictions, i.e.,

\begin{eqnarray}
\mu_{i} & = & g(\bm{\theta}, x_{i})\nonumber\\
\mathscr{L}(\bm{\theta}, \sigma^{2} \mid y_{i}) & \propto & \underbrace{[y_{i} \mid \mu_{i},\sigma^{2}]}_{\text{PDF or PMF}}\nonumber
\end{eqnarray}

\end{frame}

\begin{frame}{Likelihood Surfaces}

\centerline{\includegraphics[width=9cm]{../../Graphics/LikelihoodSurface.png}}

\vspace*{1cm}

Figure courtesy of
\href{http://nesterko.com/lectures/stat221-2012/lecture8/\#/6}{\beamerbutton{Sergiy Nesterko}}.

\end{frame}

\begin{frame}{How do we fit models with multiple parameters and multiple
data points?}

The total likelihood is the product of the individual likelihoods,
assuming the data are \emph{conditionally independent}:

\begin{eqnarray}
\mathscr{L}(\bm{\mu}, \sigma^{2} \mid \mathbf{y}) & = & c\prod_{i=1}^{n}[y_{i} \mid g(\bm{\theta}, x_{i}),\sigma^{2}]\nonumber
\end{eqnarray}

\end{frame}

\begin{frame}{What does conditionally independent mean?}

Independence is an assumption! Remember from the chain rule:

\[Pr(y_{1},\ldots,y_{n}) = Pr(y_{1} \mid y_2\dots y_{n})Pr(y_{2} \mid y_{3}\ldots y_{n})\ldots Pr(y_{n}).\]

However, by assuming that these random variables are independent, you
can simplyify the joint probability into:

\[Pr(y_{1},\ldots,y_{n}) = Pr(y_{1})Pr(y_{2})\ldots Pr(y_{n}),\]

such that the total likelihood a product of the individual likelihoods.

\end{frame}

\begin{frame}{What does conditionally independent mean?}

We evaluate the independence assumption by examining the residuals
\((\epsilon)\) from a model, where
\((\epsilon_{i}=y_{i}-g(\bm{\theta},x_{i}))\).

The independence assumption holds if knowing a residual tells you
nothing about the other residuals.

We assess this by ensuring that the residuals:

\begin{itemize}
\tightlist
\item
  do not show a trend, meaning they should be centered on 0 throughout
  the range of fitted values,
\item
  and are not be autocorrelated.
\end{itemize}

\end{frame}

\begin{frame}{Log likelihoods:}

We often use the sum of the log likelihoods to get the total log
likelihood as a basis for fitting models:

\[\log(\mathscr{L}(\bm{\theta}, \sigma^{2} \mid y))=L(\bm{\theta}, \sigma^{2} \mid y) =\log(c)+\sum_{i=1}^{n}\log([y_{i}\mid g(\bm{\theta}, x_{i}),\sigma^{2}])\]

\end{frame}

\begin{frame}{The exponential distribution}

\begin{columns}[T] % contents are top vertically aligned
\begin{column}[T]{5cm} % each column can also be its own environment
\begin{eqnarray}
t_{i} & \sim & \textrm{exponential}(\lambda)\nonumber\\
P(t_{i} \mid \lambda) & = &
\begin{cases}
\lambda e^{-\lambda t_{i}} & t_{i} \geq 0 \nonumber\\
0 & t_{i} < 0
\end{cases}
\end{eqnarray}
if $\lambda$ is the average number of events time\textsuperscript{-1}, then $1/\lambda$ is the average time between events and mean of the exponential distribution.
\end{column}

\begin{column}[T]{6cm} 



\begin{center}
\input{tempPlot6.tex}
\end{center}

\end{column}
\end{columns}

\begin{itemize}
\tightlist
\item
  The data, \(t_{i}\), represent ``waiting times'' between events
  happening in a Poisson process. If the number of events per unit time
  is provided by the Poisson distribution, then the length of time
  between events is provided by the exponential distribution.
\item
  See this
  \href{https://neurophysics.ucsd.edu/courses/physics_171/exponential.pdf}{\beamerbutton{link}}
  by John C.B. Cooper for a clear and concise explanation of the
  connection between these two distributions.
\end{itemize}

\end{frame}

\begin{frame}{Likelihood exercise for time to tweet}

\centerline{\includegraphics[width=8cm]{/Users/coldwater/Documents/TeachLearn/BayesWorkshop/Lectures/Graphics/TrumpExponential.png}}

Generate a datum quantifying how long we have to wait for a new tweet
from the POTUS. Write out the likelihood function assuming wait times
for POTUS tweets are governed by an exponential distribution. Determine
the maximum likelihood estimate (MLE) for \(\lambda\).

\end{frame}

\begin{frame}{Generate the datum to estimate \(\lambda\) for POTUS}

\begin{itemize}
\item
  \(\frac{991}{151} \sim 6.6\) tweets per day
\item
  \(t_{1} = \frac{1}{6.6} \sim 0.15\) days between each tweet
\item
  Fun fact: 3.7 hours respite between POTUS tweets!
\end{itemize}

\end{frame}

\begin{frame}{Analytically solve for \(\lambda_{MLE}\) for POTUS}

\begin{alignat*}{3}
\mathscr{L}(\lambda) & =  \vphantom{\frac11}[t_{1} \mid \lambda] &&\quad\text{The likelihood equals the PDF.}\\
\mathscr{L}(\lambda) & =  \vphantom{\frac11}\lambda e^{-\lambda t_{1}}&&\quad\text{Substitude the exponential PDF.}\\
L(\lambda) & =  \vphantom{\frac11}\log(\lambda)-\lambda{t_{1}}&&\quad\text{Take logs to ease differentiation.}\\
\frac{dL(\lambda)}{d\lambda} & = \frac{1}{\lambda}-{t_{1}}&&\quad\text{Differentiate.}\\
0 & = \frac{1}{\lambda_{MLE}}-{t_{1}}&&\quad\text{Set derivative = 0 and solve for $\lambda_{MLE}$.}\\
\lambda_{\textrm{MLE}} & =\frac{1}{{0.15}}= 6.6 &&\quad\text{Note that $\lambda_{\textrm{MLE}}$ = Poisson rate parameter.}
\end{alignat*}

\end{frame}

\begin{frame}{Likelihood profile of \(\lambda\) for POTUS}

\begin{center}
\input{tempPlot7.tex}
\end{center}

\end{frame}

\begin{frame}{Tweeting by heads of state\footnotemark}

\centerline{\includegraphics[width=12cm]{../../Graphics/G7Tweets.png}}

Assuming tweet wait times for heads of state arise from an exponential
distribution whose paramater is \(\lambda\):

\begin{itemize}
\tightlist
\item
  Write the total likelihood and log-likelihood function.
\item
  \beamerbutton{Advanced} Find the maximum likelihood estimate for
  \(\lambda\).
\item
  Plot the likelihood profile for \(\lambda\) using R.
\end{itemize}

\bigskip

\footnotetext[1]{Wait times between tweets collected with R package twitteR.}

\end{frame}

\begin{frame}{Likelihood and log likelihood functions and
\(\lambda_{MLE}\)\footnotemark}

\centerline{\includegraphics[width=12cm]{../../Graphics/G7Tweets.png}}

\vspace*{-1cm}

\begin{align*}
\mathscr{L}(\lambda) & = \prod_{i=1}^{6}[t_{i} \mid \lambda] & \frac{dL(\lambda)}{d\lambda}& = \frac{6}{\lambda}-\sum_{i=1}^{6}t_{i}\\
\Aboxed{\mathscr{L}(\lambda) & = \prod_{i=1}^{6}\lambda e^{-\lambda{t_{i}}}} & \lambda_{\textrm{MLE}} & =\frac{6}{24.9}= 0.24\\
\Aboxed{L(\lambda) & = 6\log(\lambda)-\lambda{(\sum_{i=1}^{6}t_{i})}} & \Aboxed{{\lambda_{\textrm{MLE}}} & =\frac{1}{\sum_{i=1}^{n}t_{i}/n}}
\end{align*}

\end{frame}

\begin{frame}[fragile]{Likelihood profile of \(\lambda\)}

\begin{center}
\input{tempPlot9.tex}
\end{center}

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(.}\DecValTok{01}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{1000}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\OtherTok{NA}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(lambda)) \{y[i] <-}\StringTok{ }\KeywordTok{prod}\NormalTok{(}\KeywordTok{dexp}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{22}\NormalTok{, .}\DecValTok{077}\NormalTok{, .}\DecValTok{0053}\NormalTok{, }\FloatTok{1.1}\NormalTok{), lambda[i]))\}}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}{Likelihood Surfaces}

\centerline{\includegraphics[width=9cm]{../../Graphics/LikelihoodSurface.png}}

\vspace*{1cm}

Figure courtesy of
\href{http://nesterko.com/lectures/stat221-2012/lecture8/\#/6}{\beamerbutton{Sergiy Nesterko}}.

\end{frame}

\begin{frame}{Main points}

\begin{itemize}
\tightlist
\item
  Likelihood allows us to evaluate the relative strength of evidence for
  one parameter or model relative to another.
\item
  The data are fixed and the parameters are variable in likelihood
  functions. These functions do not integrate or sum to one over the
  range of values of the parameter.
\item
  The data are variable and the parameter are fixed in probability mass
  functions and probability density functions. These functions sum or
  integrate to one over the support of the random variable, \(y\).
\end{itemize}

\end{frame}

\begin{frame}{Looking ahead: likelihood vs.~Bayes}

\begin{center}
\input{tempPlot10.tex}
\end{center}

\begin{itemize}
\tightlist
\item
  What must be done to insure that the area under the curve = 1?
\item
  If you do this, is this now a probability density function for
  \(\theta\)?
\end{itemize}

\end{frame}

\begin{frame}{Likelihood ratio confidence intervals}

Find the upper and lower bounds of an interval where all \(\lambda\)
values within that interval are as consistent with the data as
\(\lambda_{MLE}\).

We compute the likelihood ratio statistic:

\[\mathscr{R} = 2\log\Big(\frac{\mathscr{L}(\lambda_{MLE} \mid t_{1})}{\mathscr{L}(\lambda_{0}\mid t_{1})}\Big)  \sim \chi_{k=1}^{2}\]

which is distributed \(\chi^{2}\) with 1 degree of freedom. Note that we
fail to reject \(H_{0}\) that \(\lambda = \lambda_{0}\) if
\(\mathscr{R} < \chi_{k=1}^{2}(1 - \alpha)\).

\end{frame}

\begin{frame}{Likelihood ratio confidence intervals}

We determine the \((1-\alpha = 0.95)\) likelihood ratio confidence
interval by finding the upper and lower bounds for all values of
\(\lambda_{0}\) where we would fail to reject \(H_{0}\).

\begin{eqnarray*}
2\log\Big(\frac{\mathscr{L}(\lambda_{MLE} \mid t_{1})}{\mathscr{L}(\lambda\mid t_{1})}\Big) < \chi_{k=1}^{2}(0.95)\\
L(\lambda_{MLE}) - \frac{3.84}{2} < L(\lambda\mid t_{1})\\
L(\lambda\mid t_{1}) > L(\lambda_{MLE}) - 1.92
\end{eqnarray*}

\end{frame}

\begin{frame}{Likelihood ratio confidence intervals}

\begin{center}
\input{tempPlot8.tex}
\end{center}

Figure courtesy of the
\href{https://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture8.htm\#constructing}{\beamerbutton{UNC Biology Department}}.

\end{frame}

\begin{frame}[fragile]{Likelihood profile of \(\lambda\) for heads of
state}

\begin{center}
\input{tempPlot11.tex}
\end{center}

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(.}\DecValTok{03}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{1000}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\OtherTok{NA}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(lambda)) \{y[i] <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\KeywordTok{prod}\NormalTok{(}\KeywordTok{dexp}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{22}\NormalTok{, .}\DecValTok{077}\NormalTok{, .}\DecValTok{0053}\NormalTok{, }\FloatTok{1.1}\NormalTok{), lambda[i])))\}}
\end{Highlighting}
\end{Shaded}

\end{frame}

\end{document}
